{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "lJDVuWO3B1pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "CptExsPx7v1M"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizerFast, BertForTokenClassification, BertForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import TextDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "id": "0xAaLdBZE0G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# Read the JSON file\n",
        "path = \"/content/drive/MyDrive/model/B_data.json\"\n",
        "with open(path, 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Assuming the dataset is a list of dictionaries\n",
        "\n",
        "# Define a custom Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Create an instance of CustomDataset\n",
        "custom_dataset = CustomDataset(dataset)\n",
        "\n",
        "# Create a DataLoader\n",
        "batch_size = 1\n",
        "train_dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "bXc4PNVFRP4D"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = []\n",
        "for sample in train_dataloader:\n",
        "    key.append(sample['entities'])"
      ],
      "metadata": {
        "id": "sMcD55taOmXE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities = []\n",
        "for substr in key:\n",
        "    tokens = substr[0].split()\n",
        "    entities.extend(tokens)"
      ],
      "metadata": {
        "id": "RuS23pIA8o0K"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities"
      ],
      "metadata": {
        "id": "KRXA6PeKCy4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = set(entities)\n",
        "print(key)\n",
        "print(len(key))\n",
        "num_ner_labels = len(key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJB6otmMQSDG",
        "outputId": "cd06a59a-c568-4b01-8adc-98f234f9c0da"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'I-DATE', 'B-TASK', 'I-TIME', 'I-TASK', 'B-DATE', 'B-TIME', 'B-DUR', 'O', 'I-DUR'}\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = []\n",
        "for sample in train_dataloader:\n",
        "    l.extend(sample['intent'])"
      ],
      "metadata": {
        "id": "UY9epjj3RVri"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = set(l)\n",
        "print(l)\n",
        "print(len(l))\n",
        "num_intent_labels=len(l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAA0ljZPH0oL",
        "outputId": "4ae69224-eb18-43bc-a2ad-b51fc8eb7ec2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"'Set Timer'\", \"'Schedule Meeting'\", \"'Set Alarm'\", \"'Schedule Appointment'\", \"'Set Reminder'\"}\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT tokenizer and models for token classification and sequence classification\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "ner_model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_ner_labels)\n",
        "intent_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_intent_labels)"
      ],
      "metadata": {
        "id": "SZaMCTlI71p0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "523c5d4f-d732-4ff4-9254-373b04c5620a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing the dataset"
      ],
      "metadata": {
        "id": "-odMqRP2VICq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maybe I should try to set trincation and padding to False."
      ],
      "metadata": {
        "id": "n7tTgv64JFcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs = []\n",
        "\n",
        "max_sequence_length = 0  # Initialize maximum sequence length\n",
        "\n",
        "for batch in train_dataloader:\n",
        "    text = batch['text']\n",
        "    batch_tokenized = tokenizer(text, padding=True, return_offsets_mapping=True, truncation=True, return_tensors='pt')\n",
        "    input_ids = batch_tokenized['input_ids']\n",
        "    attention_mask = batch_tokenized['attention_mask']\n",
        "    offset_mapping = batch_tokenized[\"offset_mapping\"]\n",
        "\n",
        "    tokenized_inputs.append((input_ids, attention_mask, offset_mapping))\n",
        "\n",
        "    # Update maximum sequence length\n",
        "    max_sequence_length = max(max_sequence_length, input_ids.shape[1])\n",
        "\n",
        "# Pad sequences to the maximum sequence length\n",
        "for i in range(len(tokenized_inputs)):\n",
        "    input_ids, attention_mask, offset_mapping = tokenized_inputs[i]\n",
        "    pad_length = max_sequence_length - input_ids.shape[1]\n",
        "    padded_input_ids = torch.nn.functional.pad(input_ids, (0, pad_length), value=tokenizer.pad_token_id)\n",
        "    padded_attention_mask = torch.nn.functional.pad(attention_mask, (0, pad_length), value=0)  # Assuming 0 for padding mask\n",
        "    padded_start = torch.cat(\n",
        "    [offset_mapping[:, :, :1], torch.full((offset_mapping.size(0), pad_length, 1), -1, dtype=torch.long)], dim=1\n",
        "    )\n",
        "\n",
        "    # Pad end indices with -1 (or any appropriate value to signify padding)\n",
        "    padded_end = torch.cat(\n",
        "        [offset_mapping[:, :, 1:], torch.full((offset_mapping.size(0), pad_length, 1), -1, dtype=torch.long)], dim=1\n",
        "    )\n",
        "\n",
        "    # Concatenate padded start and end indices back together\n",
        "    padded_offset_mapping = torch.cat([padded_start, padded_end], dim=2)\n",
        "    tokenized_inputs[i] = (padded_input_ids, padded_attention_mask, padded_offset_mapping)\n",
        "\n",
        "# Concatenate input IDs and attention masks separately\n",
        "input_ids = torch.cat([tensor[0] for tensor in tokenized_inputs], dim=0)\n",
        "attention_mask = torch.cat([tensor[1] for tensor in tokenized_inputs], dim=0)\n",
        "offset_mapping = torch.cat([tensor[2] for tensor in tokenized_inputs], dim=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "DGFDqLQTVHcL"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5qLMVxDu0Lc",
        "outputId": "fa68aa97-e034-454c-f7ed-33328b84b857"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': '\"Set a timer for 10 minutes.\"',\n",
              "  'intent': \"'Set Timer'\",\n",
              "  'entities': 'O O O O B-DUR I-DUR'},\n",
              " {'text': '\"Remind me about the meeting at 3 PM tomorrow.\"',\n",
              "  'intent': \"'Set Reminder'\",\n",
              "  'entities': 'O O O O O O B-TIME I-TIME B-DATE'},\n",
              " {'text': '\"Schedule an appointment for next Friday at 9 AM.\"',\n",
              "  'intent': \"'Schedule Appointment'\",\n",
              "  'entities': 'O O O O B-DATE I-DATE O B-TIME I-TIME'},\n",
              " {'text': '\"Can you set a reminder for my doctor\\'s appointment on Monday?\"',\n",
              "  'intent': \"'Set Reminder'\",\n",
              "  'entities': 'O O O O O O O O O O B-DATE'},\n",
              " {'text': '\"I want to schedule a meeting for the 15th of this month at 2:30 PM.\"',\n",
              "  'intent': \"'Schedule Meeting'\",\n",
              "  'entities': 'O O O O O O O O B-DATE I-DATE I-DATE I-DATE O B-TIME I-TIME'},\n",
              " {'text': '\"Set an alarm for 7 AM.\"',\n",
              "  'intent': \"'Set Alarm'\",\n",
              "  'entities': 'O O O O B-TIME I-TIME'},\n",
              " {'text': '\"Remind me to call John in 30 minutes.\"',\n",
              "  'intent': \"'Set Reminder'\",\n",
              "  'entities': 'O O O B-TASK I-TASK O B-DUR I-DUR'},\n",
              " {'text': '\"Schedule a meeting for next Wednesday afternoon.\"',\n",
              "  'intent': \"'Schedule Meeting'\",\n",
              "  'entities': 'O O O O B-DATE I-DATE B-TIME'},\n",
              " {'text': '\"Can you set a timer for cooking for 1 hour?\"',\n",
              "  'intent': \"'Set Timer'\",\n",
              "  'entities': 'O O O O O O B-TASK O B-DUR I-DUR'},\n",
              " {'text': '\"Remind me about the project deadline at 5 PM on Friday.\"',\n",
              "  'intent': \"'Set Reminder'\",\n",
              "  'entities': 'O O O O B-TASK I-TASK O B-TIME I-TIME O B-DATE'},\n",
              " {'text': '\"Schedule a doctor\\'s appointment for March 20th at 10:30 AM.\"',\n",
              "  'intent': \"'Schedule Appointment'\",\n",
              "  'entities': 'O O O O O B-DATE I-DATE O B-TIME I-TIME'},\n",
              " {'text': '\"Set a timer for a 15-minute break.\"',\n",
              "  'intent': \"'Set Timer'\",\n",
              "  'entities': 'O O O O O B-TIME B-TASK'},\n",
              " {'text': '\"Remind me to buy groceries tomorrow morning.\"',\n",
              "  'intent': \"'Set Reminder'\",\n",
              "  'entities': 'O O O B-TASK I-TASK B-DATE B-TIME'},\n",
              " {'text': '\"Schedule a conference call for the first Monday of next month at 3 PM.\"',\n",
              "  'intent': \"'Schedule Meeting'\",\n",
              "  'entities': 'O O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O B-TIME I-TIME'}]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer()"
      ],
      "metadata": {
        "id": "q-OVKsSmujEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "offset_mapping"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2lXApEis2Vx",
        "outputId": "b237c9d8-af35-4779-8feb-78043db70040"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0,  0],\n",
              "         [ 0,  1],\n",
              "         [ 1,  4],\n",
              "         [ 5,  6],\n",
              "         [ 7, 12],\n",
              "         [13, 16],\n",
              "         [17, 19],\n",
              "         [20, 27],\n",
              "         [27, 28],\n",
              "         [28, 29],\n",
              "         [ 0,  0],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1]],\n",
              "\n",
              "        [[ 0,  0],\n",
              "         [ 0,  1],\n",
              "         [ 1,  7],\n",
              "         [ 8, 10],\n",
              "         [11, 16],\n",
              "         [17, 20],\n",
              "         [21, 28],\n",
              "         [29, 31],\n",
              "         [32, 33],\n",
              "         [34, 36],\n",
              "         [37, 45],\n",
              "         [45, 46],\n",
              "         [46, 47],\n",
              "         [ 0,  0],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1]],\n",
              "\n",
              "        [[ 0,  0],\n",
              "         [ 0,  1],\n",
              "         [ 1,  9],\n",
              "         [10, 12],\n",
              "         [13, 24],\n",
              "         [25, 28],\n",
              "         [29, 33],\n",
              "         [34, 40],\n",
              "         [41, 43],\n",
              "         [44, 45],\n",
              "         [46, 48],\n",
              "         [48, 49],\n",
              "         [49, 50],\n",
              "         [ 0,  0],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1]],\n",
              "\n",
              "        [[ 0,  0],\n",
              "         [ 0,  1],\n",
              "         [ 1,  4],\n",
              "         [ 5,  8],\n",
              "         [ 9, 12],\n",
              "         [13, 14],\n",
              "         [15, 23],\n",
              "         [24, 27],\n",
              "         [28, 30],\n",
              "         [31, 37],\n",
              "         [37, 38],\n",
              "         [38, 39],\n",
              "         [40, 51],\n",
              "         [52, 54],\n",
              "         [55, 61],\n",
              "         [61, 62],\n",
              "         [62, 63],\n",
              "         [ 0,  0],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1]],\n",
              "\n",
              "        [[ 0,  0],\n",
              "         [ 0,  1],\n",
              "         [ 1,  2],\n",
              "         [ 3,  7],\n",
              "         [ 8, 10],\n",
              "         [11, 19],\n",
              "         [20, 21],\n",
              "         [22, 29],\n",
              "         [30, 33],\n",
              "         [34, 37],\n",
              "         [38, 42],\n",
              "         [43, 45],\n",
              "         [46, 50],\n",
              "         [51, 56],\n",
              "         [57, 59],\n",
              "         [60, 61],\n",
              "         [61, 62],\n",
              "         [62, 64],\n",
              "         [65, 67],\n",
              "         [67, 68],\n",
              "         [68, 69],\n",
              "         [ 0,  0]],\n",
              "\n",
              "        [[ 0,  0],\n",
              "         [ 0,  1],\n",
              "         [ 1,  4],\n",
              "         [ 5,  7],\n",
              "         [ 8, 13],\n",
              "         [14, 17],\n",
              "         [18, 19],\n",
              "         [20, 22],\n",
              "         [22, 23],\n",
              "         [23, 24],\n",
              "         [ 0,  0],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1]],\n",
              "\n",
              "        [[ 0,  0],\n",
              "         [ 0,  1],\n",
              "         [ 1,  7],\n",
              "         [ 8, 10],\n",
              "         [11, 13],\n",
              "         [14, 18],\n",
              "         [19, 23],\n",
              "         [24, 26],\n",
              "         [27, 29],\n",
              "         [30, 37],\n",
              "         [37, 38],\n",
              "         [38, 39],\n",
              "         [ 0,  0],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1]],\n",
              "\n",
              "        [[ 0,  0],\n",
              "         [ 0,  1],\n",
              "         [ 1,  9],\n",
              "         [10, 11],\n",
              "         [12, 19],\n",
              "         [20, 23],\n",
              "         [24, 28],\n",
              "         [29, 38],\n",
              "         [39, 48],\n",
              "         [48, 49],\n",
              "         [49, 50],\n",
              "         [ 0,  0],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1]],\n",
              "\n",
              "        [[ 0,  0],\n",
              "         [ 0,  1],\n",
              "         [ 1,  4],\n",
              "         [ 5,  8],\n",
              "         [ 9, 12],\n",
              "         [13, 14],\n",
              "         [15, 20],\n",
              "         [21, 24],\n",
              "         [25, 32],\n",
              "         [33, 36],\n",
              "         [37, 38],\n",
              "         [39, 43],\n",
              "         [43, 44],\n",
              "         [44, 45],\n",
              "         [ 0,  0],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1]],\n",
              "\n",
              "        [[ 0,  0],\n",
              "         [ 0,  1],\n",
              "         [ 1,  7],\n",
              "         [ 8, 10],\n",
              "         [11, 16],\n",
              "         [17, 20],\n",
              "         [21, 28],\n",
              "         [29, 37],\n",
              "         [38, 40],\n",
              "         [41, 42],\n",
              "         [43, 45],\n",
              "         [46, 48],\n",
              "         [49, 55],\n",
              "         [55, 56],\n",
              "         [56, 57],\n",
              "         [ 0,  0],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1]],\n",
              "\n",
              "        [[ 0,  0],\n",
              "         [ 0,  1],\n",
              "         [ 1,  9],\n",
              "         [10, 11],\n",
              "         [12, 18],\n",
              "         [18, 19],\n",
              "         [19, 20],\n",
              "         [21, 32],\n",
              "         [33, 36],\n",
              "         [37, 42],\n",
              "         [43, 47],\n",
              "         [48, 50],\n",
              "         [51, 53],\n",
              "         [53, 54],\n",
              "         [54, 56],\n",
              "         [57, 59],\n",
              "         [59, 60],\n",
              "         [60, 61],\n",
              "         [ 0,  0],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1]],\n",
              "\n",
              "        [[ 0,  0],\n",
              "         [ 0,  1],\n",
              "         [ 1,  4],\n",
              "         [ 5,  6],\n",
              "         [ 7, 12],\n",
              "         [13, 16],\n",
              "         [17, 18],\n",
              "         [19, 21],\n",
              "         [21, 22],\n",
              "         [22, 28],\n",
              "         [29, 34],\n",
              "         [34, 35],\n",
              "         [35, 36],\n",
              "         [ 0,  0],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1]],\n",
              "\n",
              "        [[ 0,  0],\n",
              "         [ 0,  1],\n",
              "         [ 1,  7],\n",
              "         [ 8, 10],\n",
              "         [11, 13],\n",
              "         [14, 17],\n",
              "         [18, 27],\n",
              "         [28, 36],\n",
              "         [37, 44],\n",
              "         [44, 45],\n",
              "         [45, 46],\n",
              "         [ 0,  0],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1]],\n",
              "\n",
              "        [[ 0,  0],\n",
              "         [ 0,  1],\n",
              "         [ 1,  9],\n",
              "         [10, 11],\n",
              "         [12, 22],\n",
              "         [23, 27],\n",
              "         [28, 31],\n",
              "         [32, 35],\n",
              "         [36, 41],\n",
              "         [42, 48],\n",
              "         [49, 51],\n",
              "         [52, 56],\n",
              "         [57, 62],\n",
              "         [63, 65],\n",
              "         [66, 67],\n",
              "         [68, 70],\n",
              "         [70, 71],\n",
              "         [71, 72],\n",
              "         [ 0,  0],\n",
              "         [-1, -1],\n",
              "         [-1, -1],\n",
              "         [-1, -1]]])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_mask"
      ],
      "metadata": {
        "id": "9HB6S5C7KFUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51d5cad0-9544-4851-c2eb-6d3877aab730"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids"
      ],
      "metadata": {
        "id": "VpCPdPOoJAqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# task -> B-TASK\n",
        "# date -> B-DATE\n",
        "# duration -> B-DUR\n",
        "# days -> B_DAY\n",
        "# time -> B-TIME\n",
        "# specific_time\n",
        "encoded_ner_labels = []\n",
        "label_ner_map = {'O': 0, 'B-DATE': 1, 'I-DATE': 2, 'B-TIME': 3, 'I-TIME': 4, 'B-TASK': 5, 'I-TASK': 6, 'B-DUR': 7, 'I-DUR': 8}\n",
        "for sample in train_dataloader:\n",
        "    encoded_ner_labels.extend([label_ner_map[label] for label in sent_label.split()] for sent_label in sample['entities'])\n",
        "encoded_ner_labels"
      ],
      "metadata": {
        "id": "Y6J-hyyTpSAD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4de845d9-1783-4212-cc5f-9b0a786fdb9d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 0, 0, 7, 8],\n",
              " [0, 0, 0, 0, 0, 0, 3, 4, 1],\n",
              " [0, 0, 0, 0, 1, 2, 0, 3, 4],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 3, 4],\n",
              " [0, 0, 0, 0, 3, 4],\n",
              " [0, 0, 0, 5, 6, 0, 7, 8],\n",
              " [0, 0, 0, 0, 1, 2, 3],\n",
              " [0, 0, 0, 0, 0, 0, 5, 0, 7, 8],\n",
              " [0, 0, 0, 0, 5, 6, 0, 3, 4, 0, 1],\n",
              " [0, 0, 0, 0, 0, 1, 2, 0, 3, 4],\n",
              " [0, 0, 0, 0, 0, 3, 5],\n",
              " [0, 0, 0, 5, 6, 1, 3],\n",
              " [0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0, 3, 4]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train_dataloader:\n",
        "    print(i['entities'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db3lumxzUty7",
        "outputId": "e4f39386-a8af-47e4-883e-7b2f0111a9cb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O O O O B-DUR I-DUR']\n",
            "['O O O O O O B-TIME I-TIME B-DATE']\n",
            "['O O O O B-DATE I-DATE O B-TIME I-TIME']\n",
            "['O O O O O O O O O O B-DATE']\n",
            "['O O O O O O O O B-DATE I-DATE I-DATE I-DATE O B-TIME I-TIME']\n",
            "['O O O O B-TIME I-TIME']\n",
            "['O O O B-TASK I-TASK O B-DUR I-DUR']\n",
            "['O O O O B-DATE I-DATE B-TIME']\n",
            "['O O O O O O B-TASK O B-DUR I-DUR']\n",
            "['O O O O B-TASK I-TASK O B-TIME I-TIME O B-DATE']\n",
            "['O O O O O B-DATE I-DATE O B-TIME I-TIME']\n",
            "['O O O O O B-TIME B-TASK']\n",
            "['O O O B-TASK I-TASK B-DATE B-TIME']\n",
            "['O O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O B-TIME I-TIME']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "sample_entities = []\n",
        "for sample in train_dataloader:\n",
        "    sample_entities.extend(sample['entities'])\n",
        "\n",
        "# Flatten the list of strings into a single list of tokens\n",
        "all_entities = [entity for sample in sample_entities for entity in sample.split()]\n",
        "print(all_entities)\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit LabelEncoder to all unique entity labels\n",
        "label_encoder.fit(all_entities)\n",
        "\n",
        "# Encode entities for each sample in sample_entities\n",
        "encoded_entities = [\n",
        "    label_encoder.transform(entity.split()).tolist() for entity in sample_entities\n",
        "]\n",
        "\n",
        "print(encoded_entities)\n"
      ],
      "metadata": {
        "id": "TdU2s6l0pUHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95bad47b-bd89-4144-fcac-8adbafbac573"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'O', 'O', 'O', 'B-DUR', 'I-DUR', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'I-TIME', 'B-DATE', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'B-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'B-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'B-TIME', 'I-TIME', 'O', 'O', 'O', 'B-TASK', 'I-TASK', 'O', 'B-DUR', 'I-DUR', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TASK', 'O', 'B-DUR', 'I-DUR', 'O', 'O', 'O', 'O', 'B-TASK', 'I-TASK', 'O', 'B-TIME', 'I-TIME', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'B-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'B-TASK', 'O', 'O', 'O', 'B-TASK', 'I-TASK', 'B-DATE', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'B-TIME', 'I-TIME']\n",
            "[[8, 8, 8, 8, 1, 5], [8, 8, 8, 8, 8, 8, 3, 7, 0], [8, 8, 8, 8, 0, 4, 8, 3, 7], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0], [8, 8, 8, 8, 8, 8, 8, 8, 0, 4, 4, 4, 8, 3, 7], [8, 8, 8, 8, 3, 7], [8, 8, 8, 2, 6, 8, 1, 5], [8, 8, 8, 8, 0, 4, 3], [8, 8, 8, 8, 8, 8, 2, 8, 1, 5], [8, 8, 8, 8, 2, 6, 8, 3, 7, 8, 0], [8, 8, 8, 8, 8, 0, 4, 8, 3, 7], [8, 8, 8, 8, 8, 3, 2], [8, 8, 8, 2, 6, 0, 3], [8, 8, 8, 8, 8, 8, 0, 4, 4, 4, 4, 8, 3, 7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_intent_labels = []\n",
        "for sample in train_dataloader:\n",
        "    sample_intent_labels.extend(sample['intent'])\n",
        "\n",
        "label_encoder.fit(sample_intent_labels)\n",
        "\n",
        "encoded_intent_labels = label_encoder.transform(sample_intent_labels)\n",
        "\n",
        "encoded_intent_labels"
      ],
      "metadata": {
        "id": "_fSSwvVtI1MV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd8076a8-4b67-44af-94e3-4756f5245769"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 3, 0, 3, 1, 2, 3, 1, 4, 3, 0, 4, 3, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenizedDataset(Dataset):\n",
        "    def __init__(self, tokenized_inputs, attention_mask, encoded_ner_labels, encoded_intent_labels):\n",
        "        self.tokenized_inputs = tokenized_inputs\n",
        "        self.encoded_ner_labels = encoded_ner_labels\n",
        "        self.encoded_intent_labels = encoded_intent_labels\n",
        "        self.attention_mask = attention_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inputs = self.tokenized_inputs[idx]\n",
        "        ner_labels = self.encoded_ner_labels[idx]\n",
        "        intent_labels = self.encoded_intent_labels[idx]\n",
        "        attention_mask = self.attention_mask[idx]\n",
        "\n",
        "        return {\n",
        "            'inputs': inputs,\n",
        "            'ner_labels': ner_labels,\n",
        "            'intent_labels': intent_labels,\n",
        "            'attention_mask': attention_mask\n",
        "        }"
      ],
      "metadata": {
        "id": "LqdVj_pZo6P6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cust_dataset = TokenizedDataset(input_ids, attention_mask, encoded_ner_labels, encoded_intent_labels)\n",
        "data_loader = DataLoader(cust_dataset, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "FSIViFHI1b48"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ner_model architecture\n",
        "ner_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TafoSlh3S8Bn",
        "outputId": "99626944-db75-4499-c23b-708e8806fbfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intent_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6niOzBtQS_Am",
        "outputId": "aace6bb3-584e-4a82-b0b1-43d9d6125bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in data_loader:\n",
        "    print(sample['ner_labels'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO7vbAMi0R7H",
        "outputId": "34541049-e532-4d9c-c469-5054fc4ed288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([3]), tensor([4]), tensor([1])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([2]), tensor([0]), tensor([3]), tensor([4])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([7]), tensor([8])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([2]), tensor([2]), tensor([2]), tensor([0]), tensor([3]), tensor([4])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([5]), tensor([6]), tensor([0]), tensor([3]), tensor([4]), tensor([0]), tensor([1])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([5]), tensor([6]), tensor([0]), tensor([7]), tensor([8])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([2]), tensor([0]), tensor([3]), tensor([4])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([5]), tensor([0]), tensor([7]), tensor([8])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([3]), tensor([4])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([3]), tensor([5])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([0]), tensor([3]), tensor([4])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([2]), tensor([3])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([5]), tensor([6]), tensor([1]), tensor([3])]\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam([\n",
        "    {'params': ner_model.parameters()},\n",
        "    {'params': intent_model.parameters()}\n",
        "], lr=1e-5)  # Define optimizer for both models\n",
        "\n",
        "num_epochs = 3  # Define number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in data_loader:  # Iterate over your dataset batches\n",
        "\n",
        "        # Forward pass for intent classification\n",
        "        outputs_intent = intent_model(batch['inputs'])\n",
        "        intent_labels = batch['intent_labels']\n",
        "\n",
        "        intent_loss = torch.nn.CrossEntropyLoss()(outputs_intent.logits, intent_labels)\n",
        "\n",
        "        # Forward pass for NER\n",
        "        print(batch['ner_labels'])\n",
        "\n",
        "        # check whether ner_loss exists\n",
        "        ner_loss, outputs_ner_list = ner_model(\n",
        "            input_ids=batch['inputs'],\n",
        "            attention_mask=batch['attention_mask'],\n",
        "            labels=batch['ner_labels']\n",
        "        )\n",
        "        print(ner_loss)\n",
        "        # Calculate NER loss for each output tensor separately\n",
        "        ner_losses = []\n",
        "\n",
        "        print(outputs_ner_list.logits)\n",
        "\n",
        "        ner_loss = torch.nn.CrossEntropyLoss()(outputs_ner_list.logits.view(-1, num_ner_labels), batch['ner_labels'].view(-1))\n",
        "        ner_losses.append(ner_loss)\n",
        "\n",
        "        # Sum all individual NER losses\n",
        "        ner_loss = sum(ner_losses)\n",
        "\n",
        "        # Total loss (you might adjust weights for different tasks)\n",
        "        total_loss = ner_loss + intent_loss\n",
        "\n",
        "        # Backpropagation\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "h9JGFaBu8wGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam([\n",
        "    {'params': ner_model.parameters()},\n",
        "    {'params': intent_model.parameters()}\n",
        "], lr=1e-5)  # Define optimizer for both models\n",
        "\n",
        "num_epochs = 3  # Define number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:  # Iterate over your dataset batches\n",
        "        inputs = tokenizer(batch['text'], return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "        # Forward pass for intent classification\n",
        "        outputs_intent = intent_model(**inputs)\n",
        "        intent_labels = batch['intent']\n",
        "        intent_loss = torch.nn.CrossEntropyLoss()(outputs_intent.logits, intent_labels)\n",
        "\n",
        "        # Forward pass for NER\n",
        "        outputs_ner = ner_model(**inputs)\n",
        "        ner_labels = batch['entities']\n",
        "        ner_loss = torch.nn.CrossEntropyLoss()(outputs_ner.logits.view(-1, num_ner_labels), ner_labels.view(-1))\n",
        "\n",
        "        # Total loss (you might adjust weights for different tasks)\n",
        "        total_loss = ner_loss + intent_loss\n",
        "\n",
        "        # Backpropagation\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        # No idea yet"
      ],
      "metadata": {
        "id": "MZBdetDs9xfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the Multi-Task Architecture"
      ],
      "metadata": {
        "id": "OZ5Jw2arAaVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTaskBertWrapper(nn.Module):\n",
        "    def __init__(self, ner_model, intent_model):\n",
        "        super().__init__()\n",
        "        self.ner_model = ner_model\n",
        "        self.intent_model = intent_model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None, task_type=None):\n",
        "        if task_type = 'ner':\n",
        "            self.ner_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        elif task_type = 'intent':\n",
        "            self.intent_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        else:\n",
        "            raise ValueError('Invalid task_type. Use \"ner\" or \"intent\"')"
      ],
      "metadata": {
        "id": "afndjDPHAeHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiTaskBertWrapper(ner_model, intent_model)"
      ],
      "metadata": {
        "id": "tLfiD2uA50ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "5coZ3SS3_ryJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = list(model.ner_model.parameters()) + list(model.intent_model.parameters())\n",
        "optimizer = torch.optim.AdamW(params, lr=1e-5)\n",
        "\n",
        "ner_loss_function = torch.nn.CrossEntropyLoss()\n",
        "intent_loss_function = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "YSzomyoc_s7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        inputs = batch['text']\n",
        "        attention_masks = batch['attention_mask']\n",
        "        labels_ner = batch['entities']\n",
        "        labels_intent = batch['intent']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass for NER task\n",
        "        ner_outputs = model.ner_model(input_ids=inputs, attention_mask=attention_masks, labels=labels_ner, task_type='ner')\n",
        "        ner_loss = ner_loss_function(ner_outputs.logits.view(-1, num_ner_labels), labels_ner.view(-1))\n",
        "\n",
        "        ner_loss.backward()\n",
        "\n",
        "        # Forward pass for Intent Classification task\n",
        "        intent_outputs = model.intent_model(input_ids=inputs, attention_mask=attention_masks, labels=labels_intent, task_type='intent')\n",
        "        intent_loss = intent_loss_function(intent_outputs.logits.view(-1, num_intent_labels), labels_intent.view(-1))\n",
        "\n",
        "        intent_loss.backward()\n",
        "\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "-kwfmmiGAKtX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b9zbC6ABPBS"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece] seqeval[gpu]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jnGRzmhqBWF1"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizerFast, BertForTokenClassification, BertForSequenceClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import TextDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9ZtmdcxBYMs"
      },
      "outputs": [],
      "source": [
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vxqkLzucfiG_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# Read the JSON file\n",
        "path = \"/content/drive/MyDrive/model/B_data.json\"\n",
        "with open(path, 'r') as f:\n",
        "    dataset = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/model/training_set.json\"\n",
        "with open(path, 'r') as f:\n",
        "    test_dataset = json.load(f)"
      ],
      "metadata": {
        "id": "_ZTqlfb_qrBo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JQin9IoNfnh-"
      },
      "outputs": [],
      "source": [
        "text, intent, ner = [], [], []\n",
        "for i in dataset:\n",
        "    text.append(i['text'])\n",
        "    intent.append(i['intent'])\n",
        "    ner.append(i['entities'].split())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text, test_intent, test_ner = [], [], []\n",
        "for i in test_dataset:\n",
        "    test_text.append(i['text'])\n",
        "    test_intent.append(i['intent'])\n",
        "    test_ner.append(i['entities'].split())"
      ],
      "metadata": {
        "id": "j8zDpSSQq1KY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw-u5fu62PwD",
        "outputId": "6f3e628b-44e0-4da8-ff86-d5ec91ad3f6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O', 'O', 'O', 'O', 'B-DUR', 'I-DUR']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "o = ner[0]\n",
        "o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLPHaXjv1-NC",
        "outputId": "6edc0906-2798-4664-e139-ac8b13694f70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Set', 'a', 'timer', 'for', '10', 'minutes.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "o = text[0].strip().split()\n",
        "o"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just test"
      ],
      "metadata": {
        "id": "LU2rkg73rrvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_ner[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3RdWsCKtzx9",
        "outputId": "d4b1bdb8-d92f-4676-c39b-af5603eb2069"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'B-TASK',\n",
              " 'I-TASK',\n",
              " 'B-DATE',\n",
              " 'I-DATE',\n",
              " 'O',\n",
              " 'B-TIME',\n",
              " 'I-TIME']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_ = set(test_intent)\n",
        "num_i = len(unique_)\n",
        "\n",
        "unique_, num_i"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8YtM-6frklA",
        "outputId": "e835b997-9e8a-4ec9-f33d-f70713ffa90a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({\"'Schedule Appointment'\",\n",
              "  \"'Schedule Meeting'\",\n",
              "  \"'Set Alarm'\",\n",
              "  \"'Set Reminder'\",\n",
              "  \"'Set Timer'\"},\n",
              " 5)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_r = [tag for subset in test_ner for tag in subset]\n",
        "uni = set(one_r)\n",
        "num_ = len(uni)\n",
        "uni, num_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ4AXcsfrvsm",
        "outputId": "a438ddae-c07e-4f24-f4e4-58091d548a7b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'B-DATE',\n",
              "  'B-DUR',\n",
              "  'B-TASK',\n",
              "  'B-TIME',\n",
              "  'I-DATE',\n",
              "  'I-DUR',\n",
              "  'I-TASK',\n",
              "  'I-TIME',\n",
              "  'O'},\n",
              " 9)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delete what is above"
      ],
      "metadata": {
        "id": "TbImDj_wr6iN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTUJWv92f3sx",
        "outputId": "cd2bf3c3-2c37-4b13-fe85-0483510d1c80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({\"'Schedule Appointment'\",\n",
              "  \"'Schedule Meeting'\",\n",
              "  \"'Set Alarm'\",\n",
              "  \"'Set Reminder'\",\n",
              "  \"'Set Timer'\"},\n",
              " 5)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "unique_intents = set(intent)\n",
        "num_intent_labels = len(unique_intents)\n",
        "\n",
        "unique_intents, num_intent_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdoGreyqgosK",
        "outputId": "d8b31fd9-77e9-4ded-ea44-242dd205b14c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'B-DATE',\n",
              "  'B-DUR',\n",
              "  'B-TASK',\n",
              "  'B-TIME',\n",
              "  'I-DATE',\n",
              "  'I-DUR',\n",
              "  'I-TASK',\n",
              "  'I-TIME',\n",
              "  'O'},\n",
              " 9)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "one_dimensional_ner = [tag for subset in ner for tag in subset ]\n",
        "unique_ner = set(one_dimensional_ner)\n",
        "num_ner_labels = len(unique_ner)\n",
        "unique_ner, num_ner_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDSp3FM-gtCb"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "ner_model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_ner_labels)\n",
        "intent_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_intent_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gIzCQacBpFzx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1313cc9a-989e-4954-d2fb-70a910f804b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'O',\n",
              " 1: 'B-DATE',\n",
              " 2: 'I-DATE',\n",
              " 3: 'B-TIME',\n",
              " 4: 'I-TIME',\n",
              " 5: 'B-TASK',\n",
              " 6: 'I-TASK',\n",
              " 7: 'B-DUR',\n",
              " 8: 'I-DUR'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "labels_to_ids_ner = {\n",
        "    'O': 0,\n",
        "    'B-DATE': 1,\n",
        "    'I-DATE': 2,\n",
        "    'B-TIME': 3,\n",
        "    'I-TIME': 4,\n",
        "    'B-TASK': 5,\n",
        "    'I-TASK': 6,\n",
        "    'B-DUR': 7,\n",
        "    'I-DUR': 8\n",
        "    }\n",
        "\n",
        "ids_to_labels_ner = {v: k for k, v in labels_to_ids_ner.items()}\n",
        "ids_to_labels_ner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "t3y8NTdexh0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5260fa5e-b98e-4d44-f7a2-f31282c8a83d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: \"'Schedule Appointment'\",\n",
              " 1: \"'Schedule Meeting'\",\n",
              " 2: \"'Set Alarm'\",\n",
              " 3: \"'Set Reminder'\",\n",
              " 4: \"'Set Timer'\"}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "labels_to_ids_intent = {\n",
        "    \"'Schedule Appointment'\": 0,\n",
        "    \"'Schedule Meeting'\": 1,\n",
        "    \"'Set Alarm'\": 2,\n",
        "    \"'Set Reminder'\": 3,\n",
        "    \"'Set Timer'\": 4\n",
        "}\n",
        "\n",
        "ids_to_labels_intent = {v: k for k, v in labels_to_ids_intent.items()}\n",
        "ids_to_labels_intent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QvkVdQRVjuLM"
      },
      "outputs": [],
      "source": [
        "class dataset(Dataset):\n",
        "    def __init__(self, text, intent, ner, tokenizer, max_len=128):\n",
        "        self.len = len(text)\n",
        "        self.text = text\n",
        "        self.intent = intent\n",
        "        self.ner = ner\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # step 1: get the sentence, ner label, and intent_label\n",
        "        sentence = self.text[index].strip()\n",
        "        intent_label = self.intent[index].strip()\n",
        "        ner_labels = self.ner[index]\n",
        "\n",
        "        # step 2: use tokenizer to encode a sentence (includes padding/truncation up to max length)\n",
        "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" which highlights where each token starts and ends\n",
        "        encoding = self.tokenizer(\n",
        "            sentence,\n",
        "            return_offsets_mapping=True,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_len\n",
        "        )\n",
        "\n",
        "        # step 3: create ner token labels only for first word pieces of each tokenized word\n",
        "        tokenized_ner_labels = [labels_to_ids_ner[label] for label in ner_labels]\n",
        "        # create an empty array of -100 of length max_length\n",
        "        encoded_ner_labels = np.ones(len(encoding['offset_mapping']), dtype=int) * -100\n",
        "\n",
        "        # set only labels whose first offset position is 0 and the second is not 0\n",
        "        i = 0\n",
        "        prev = -1\n",
        "        for idx, mapping in enumerate(encoding['offset_mapping']):\n",
        "            if mapping[0] == mapping[1] == 0:\n",
        "                continue\n",
        "            if mapping[0] != prev:\n",
        "                # overwrite label\n",
        "                encoded_ner_labels[idx] = tokenized_ner_labels[i]\n",
        "                prev = mapping[1]\n",
        "                i += 1\n",
        "            else:\n",
        "                prev = mapping[1]\n",
        "\n",
        "        # create intent token labels\n",
        "        tokenized_intent_label = labels_to_ids_intent[intent_label]\n",
        "\n",
        "        # step 4: turn everything into Pytorch tensors\n",
        "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
        "        item['ner_labels'] = torch.as_tensor(encoded_ner_labels)\n",
        "        item['intent_labels'] = torch.as_tensor(tokenized_intent_label)\n",
        "\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cEaNyuELzNWa"
      },
      "outputs": [],
      "source": [
        "training_set = dataset(text, intent, ner, tokenizer)\n",
        "test_set = dataset(test_text, test_intent, test_ner, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gqvwYgr0jfS"
      },
      "outputs": [],
      "source": [
        "training_set[5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbBNIl17mTVH"
      },
      "source": [
        "Let us verify that the input ids and corresponding targets are correct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5Tf1jeZb0k-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8f3709b-d30f-4245-a5a3-c31b462d4180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] -- -100\n",
            "schedule -- 0\n",
            "a -- 0\n",
            "dentist -- 5\n",
            "appointment -- 6\n",
            "for -- 0\n",
            "april -- 1\n",
            "5th -- 2\n",
            "at -- 0\n",
            "11 -- 3\n",
            ": -- -100\n",
            "00 -- -100\n",
            "in -- 4\n",
            "the -- 4\n",
            "morning -- 4\n",
            ". -- -100\n",
            "[SEP] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n",
            "[PAD] -- -100\n"
          ]
        }
      ],
      "source": [
        "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[20]['input_ids']), training_set[20]['ner_labels']):\n",
        "    print(f\"{token} -- {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd80tO8InMUM"
      },
      "source": [
        "# I HAVE TO GET RID OF \" IN MY DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "k8gK0rcxnQgi"
      },
      "outputs": [],
      "source": [
        "# The dataset is small, batch_size of 1 would not impact the training time significantly\n",
        "training_loader = DataLoader(training_set, batch_size=1)\n",
        "test_loader = DataLoader(test_set, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tjIswaA6ojMQ"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "xy1OOPFKZ4fs",
        "outputId": "859609e7-a477-4464-8e19-14442046b7b4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ner_model.to(device)\n",
        "intent_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_S3mmSta5T0",
        "outputId": "55b28791-5567-4ca2-b235-1ce679ee521c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf352WfopY0y"
      },
      "source": [
        "The initial loss of the model should be close to -ln(1/num_labels)=-ln(1/9). In this case it is 2.20.\n",
        "Why? Because we are using cross entropy loss. The cross entropy loss is defined as -ln(probability score of the model for the correct class). In the beginning, the weights are random, so the probability distribution for all of the classes for a given token will be uniform, meaning that the probability for the correct class will be near 1/9. The loss for a given token will thus be -ln(1/9). As PyTorch's CrossEntropyLoss (which is used by BertForTokenClassification) uses mean reduction by default, it will compute the mean loss for each of the tokens in the sequence for which a label is provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jjk1I1coLLI",
        "outputId": "fcf78e33-50a3-4ca6-f30d-5d393c38a7f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.2205, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "inputs = training_set[2]\n",
        "input_ids = inputs[\"input_ids\"].unsqueeze(0)\n",
        "attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n",
        "labels = inputs[\"ner_labels\"].unsqueeze(0)\n",
        "\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "outputs = ner_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "initial_loss = outputs[0]\n",
        "initial_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iJ_clHk6ldm"
      },
      "source": [
        "The shape of logits must be __[batch_size, sequence_length, num_labels]__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "619BrUfmqBKS",
        "outputId": "d5cf254d-bed2-4d60-a30d-164d7f4b5f55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "tr_logits = outputs[1]\n",
        "tr_logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDZfSrT-7RdO"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam([\n",
        "    {'params': ner_model.parameters()},\n",
        "    {'params': intent_model.parameters()}\n",
        "], lr=1e-5)"
      ],
      "metadata": {
        "id": "0Jfa3o8tUfLU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TxB_EoOXvWm"
      },
      "source": [
        "# Just testing the models; delete it after finished"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kISPCk3X1VU"
      },
      "source": [
        "____"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "F_PxBooK8cdo"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    tr_ner_loss, tr_ner_accuracy = 0, 0\n",
        "    tr_intent_loss, tr_intent_accuracy = 0, 0\n",
        "    nb_tr_steps = 0\n",
        "    tr_ner_preds, tr_ner_labels = [], []\n",
        "    tr_intent_labels, tr_intent_predictions = [], []\n",
        "    ner_model.train()\n",
        "    intent_model.train()\n",
        "\n",
        "    for idx, batch in enumerate(training_loader):\n",
        "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "        ner_labels = batch['ner_labels'].to(device, dtype=torch.long)\n",
        "        intent_labels = batch['intent_labels'].to(device, dtype=torch.long)\n",
        "\n",
        "        ner_logits = ner_model(input_ids=ids, attention_mask=mask, labels=ner_labels)\n",
        "\n",
        "        # here we train an intent_model\n",
        "        intent_logits = intent_model(input_ids=ids, attention_mask=mask, labels=intent_labels)\n",
        "\n",
        "        ner_loss = ner_logits.loss\n",
        "        intent_loss = intent_logits.loss\n",
        "\n",
        "        comb_loss = ner_loss + intent_loss\n",
        "        # till here\n",
        "\n",
        "        tr_ner_loss += ner_logits['loss']\n",
        "        tr_intent_loss += intent_logits['loss']\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "        if idx % 5 == 0:\n",
        "            loss_ner_step = tr_ner_loss / nb_tr_steps\n",
        "            loss_intent_step = tr_intent_loss / nb_tr_steps\n",
        "            print(f\"Training NER loss per {idx} training steps: {loss_ner_step}\")\n",
        "            print(f\"Training INTENT loss per {idx} training steps: {loss_intent_step}\")\n",
        "\n",
        "        # compute training accuracy (FOR NER)\n",
        "        flattened_ner_targets = ner_labels.view(-1) # shape (batch_size * seq_len)\n",
        "        active_ner_logits = ner_logits.logits.view(-1, ner_model.num_labels) # shape (batch_size*seq_len, num_labels)\n",
        "        flattened_ner_predictions = torch.argmax(active_ner_logits, axis=1) # shape (batch_size * seq_len)\n",
        "\n",
        "        # compute accuracy only at active labels\n",
        "        active_ner_accuracy = ner_labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        ac_ner_labels = torch.masked_select(flattened_ner_targets, active_ner_accuracy)\n",
        "        ner_predictions = torch.masked_select(flattened_ner_predictions, active_ner_accuracy)\n",
        "\n",
        "        tr_ner_labels.extend(ac_ner_labels)\n",
        "        tr_ner_preds.extend(ner_predictions)\n",
        "\n",
        "        # compute accuracy for intent_model\n",
        "        # I CAN MAKE THE CALCULATION MUCH EASIER\n",
        "        # FIGURE IT OUT\n",
        "        flattened_intent_targets = intent_labels.view(-1)\n",
        "        active_intent_logits = intent_logits.logits.view(-1, intent_model.num_labels)\n",
        "        flattened_intent_predictions = torch.argmax(active_intent_logits, axis=1)\n",
        "\n",
        "        sample_intent_accuracy = intent_labels.view(-1)\n",
        "        active_intent_accuracy = torch.ones_like(sample_intent_accuracy, dtype=torch.bool)\n",
        "\n",
        "        ac_intent_labels = torch.masked_select(flattened_intent_targets, active_intent_accuracy)\n",
        "        intent_predictions = torch.masked_select(flattened_intent_predictions, active_intent_accuracy)\n",
        "\n",
        "        tr_intent_labels.extend(ac_intent_labels)\n",
        "        tr_intent_predictions.extend(intent_predictions)\n",
        "\n",
        "        tmp_tr_ner_accuracy = accuracy_score(ac_ner_labels.cpu().numpy(), ner_predictions.cpu().numpy())\n",
        "        tmp_tr_intent_accuracy = accuracy_score(ac_intent_labels.cpu().numpy(), intent_predictions.cpu().numpy())\n",
        "\n",
        "\n",
        "        tr_ner_accuracy += tmp_tr_ner_accuracy\n",
        "        tr_intent_accuracy += tmp_tr_intent_accuracy\n",
        "\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=ner_model.parameters(), max_norm=10\n",
        "        )\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=intent_model.parameters(), max_norm=10\n",
        "        )\n",
        "\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        comb_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = (tr_ner_loss + tr_intent_loss) / nb_tr_steps\n",
        "    tr_ner_accuracy = tr_ner_accuracy / nb_tr_steps\n",
        "    tr_intent_accuracy = tr_intent_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training NER accuracy epoch: {tr_ner_accuracy}\")\n",
        "    print(f\"Training INTENT accuracy epoch: {tr_intent_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3):\n",
        "    print(f\"Training epoch: {epoch+1}\")\n",
        "    print(\"----------------------------\")\n",
        "    train(epoch)"
      ],
      "metadata": {
        "id": "JHz3SgpIfsxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ce6e2de-e52a-48ca-a8c0-44b281c03269"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch: 1\n",
            "----------------------------\n",
            "Training NER loss per 0 training steps: 2.292585611343384\n",
            "Training INTENT loss per 0 training steps: 1.070138692855835\n",
            "Training NER loss per 5 training steps: 2.065725088119507\n",
            "Training INTENT loss per 5 training steps: 1.6158603429794312\n",
            "Training NER loss per 10 training steps: 1.9141026735305786\n",
            "Training INTENT loss per 10 training steps: 1.5342868566513062\n",
            "Training NER loss per 15 training steps: 1.7891895771026611\n",
            "Training INTENT loss per 15 training steps: 1.4432443380355835\n",
            "Training NER loss per 20 training steps: 1.7322863340377808\n",
            "Training INTENT loss per 20 training steps: 1.4711107015609741\n",
            "Training NER loss per 25 training steps: 1.6883087158203125\n",
            "Training INTENT loss per 25 training steps: 1.4129210710525513\n",
            "Training NER loss per 30 training steps: 1.639518141746521\n",
            "Training INTENT loss per 30 training steps: 1.4211585521697998\n",
            "Training loss epoch: 2.9790008068084717\n",
            "Training NER accuracy epoch: 0.48376409304980733\n",
            "Training INTENT accuracy epoch: 0.5428571428571428\n",
            "Training epoch: 2\n",
            "----------------------------\n",
            "Training NER loss per 0 training steps: 0.9474005699157715\n",
            "Training INTENT loss per 0 training steps: 0.8799883127212524\n",
            "Training NER loss per 5 training steps: 0.8535735011100769\n",
            "Training INTENT loss per 5 training steps: 1.1996008157730103\n",
            "Training NER loss per 10 training steps: 0.9435178637504578\n",
            "Training INTENT loss per 10 training steps: 1.247714877128601\n",
            "Training NER loss per 15 training steps: 0.9619663953781128\n",
            "Training INTENT loss per 15 training steps: 1.1523497104644775\n",
            "Training NER loss per 20 training steps: 0.9854699969291687\n",
            "Training INTENT loss per 20 training steps: 1.1808489561080933\n",
            "Training NER loss per 25 training steps: 0.9940418601036072\n",
            "Training INTENT loss per 25 training steps: 1.1076956987380981\n",
            "Training NER loss per 30 training steps: 0.9902365207672119\n",
            "Training INTENT loss per 30 training steps: 1.0880364179611206\n",
            "Training loss epoch: 2.0538039207458496\n",
            "Training NER accuracy epoch: 0.6978630100058673\n",
            "Training INTENT accuracy epoch: 0.6571428571428571\n",
            "Training epoch: 3\n",
            "----------------------------\n",
            "Training NER loss per 0 training steps: 0.5673689842224121\n",
            "Training INTENT loss per 0 training steps: 0.5063186883926392\n",
            "Training NER loss per 5 training steps: 0.47645530104637146\n",
            "Training INTENT loss per 5 training steps: 1.1676913499832153\n",
            "Training NER loss per 10 training steps: 0.585486114025116\n",
            "Training INTENT loss per 10 training steps: 1.0922305583953857\n",
            "Training NER loss per 15 training steps: 0.5995115041732788\n",
            "Training INTENT loss per 15 training steps: 0.9763868451118469\n",
            "Training NER loss per 20 training steps: 0.6488369703292847\n",
            "Training INTENT loss per 20 training steps: 0.9352244138717651\n",
            "Training NER loss per 25 training steps: 0.6698668599128723\n",
            "Training INTENT loss per 25 training steps: 0.8740770816802979\n",
            "Training NER loss per 30 training steps: 0.6716185808181763\n",
            "Training INTENT loss per 30 training steps: 0.8363621234893799\n",
            "Training loss epoch: 1.492553472518921\n",
            "Training NER accuracy epoch: 0.8364398300112588\n",
            "Training INTENT accuracy epoch: 0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the model"
      ],
      "metadata": {
        "id": "GIBPDraRgRps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(ner_model, intent_model, test_loader):\n",
        "    ner_model.eval()\n",
        "    intent_model.eval()\n",
        "\n",
        "    eval_ner_loss, eval_ner_accuracy = 0, 0\n",
        "    eval_intent_loss, eval_intent_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_ner_preds, eval_ner_labels = [], []\n",
        "    eval_intent_preds, eval_intent_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(test_loader):\n",
        "            ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "            ner_labels = batch['ner_labels'].to(device, dtype=torch.long)\n",
        "            intent_labels = batch['intent_labels'].to(device, dtype=torch.long)\n",
        "\n",
        "            ner_logits = ner_model(input_ids=ids, attention_mask=mask, labels=ner_labels)\n",
        "            intent_logits = intent_model(input_ids=ids, attention_mask=mask, labels=intent_labels)\n",
        "\n",
        "            eval_ner_loss += ner_logits['loss']\n",
        "            eval_intent_loss += intent_logits['loss']\n",
        "            nb_eval_steps += 1\n",
        "\n",
        "            if idx % 5 == 0:\n",
        "                loss_ner_step = eval_ner_loss / nb_eval_steps\n",
        "                loss_intent_step = eval_intent_loss / nb_eval_steps\n",
        "                print(f\"Validation NER loss per {idx} training steps: {loss_ner_step}\")\n",
        "                print(f\"Validation INTENT loss per {idx} training steps: {loss_intent_step}\")\n",
        "\n",
        "            # compute training accuracy (FOR NER)\n",
        "            flattened_ner_targets = ner_labels.view(-1) # shape (batch_size * seq_len)\n",
        "            active_ner_logits = ner_logits.logits.view(-1, ner_model.num_labels) # shape (batch_size*seq_len, num_labels)\n",
        "            flattened_ner_predictions = torch.argmax(active_ner_logits, axis=1) # shape (batch_size * seq_len)\n",
        "\n",
        "            # compute accuracy only at active labels\n",
        "            active_ner_accuracy = ner_labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "            ac_ner_labels = torch.masked_select(flattened_ner_targets, active_ner_accuracy)\n",
        "            ner_predictions = torch.masked_select(flattened_ner_predictions, active_ner_accuracy)\n",
        "\n",
        "            eval_ner_labels.extend(ac_ner_labels)\n",
        "            eval_ner_preds.extend(ner_predictions)\n",
        "\n",
        "            # compute accuracy for intent_model\n",
        "            # I CAN MAKE THE CALCULATION MUCH EASIER\n",
        "            # FIGURE IT OUT\n",
        "            flattened_intent_targets = intent_labels.view(-1)\n",
        "            active_intent_logits = intent_logits.logits.view(-1, intent_model.num_labels)\n",
        "            flattened_intent_predictions = torch.argmax(active_intent_logits, axis=1)\n",
        "\n",
        "            sample_intent_accuracy = intent_labels.view(-1)\n",
        "            active_intent_accuracy = torch.ones_like(sample_intent_accuracy, dtype=torch.bool)\n",
        "\n",
        "            ac_intent_labels = torch.masked_select(flattened_intent_targets, active_intent_accuracy)\n",
        "            intent_predictions = torch.masked_select(flattened_intent_predictions, active_intent_accuracy)\n",
        "\n",
        "            eval_intent_labels.extend(ac_intent_labels)\n",
        "            eval_intent_preds.extend(intent_predictions)\n",
        "\n",
        "            tmp_eval_ner_accuracy = accuracy_score(ac_ner_labels.cpu().numpy(), ner_predictions.cpu().numpy())\n",
        "            tmp_eval_intent_accuracy = accuracy_score(ac_intent_labels.cpu().numpy(), intent_predictions.cpu().numpy())\n",
        "\n",
        "            eval_ner_accuracy += tmp_eval_ner_accuracy\n",
        "            eval_intent_accuracy += tmp_eval_intent_accuracy\n",
        "\n",
        "    v_ner_labels = [ids_to_labels_ner[id.item()] for id in eval_ner_labels]\n",
        "    v_ner_predictions = [ids_to_labels_ner[id.item()] for id in eval_ner_preds]\n",
        "\n",
        "    v_intent_labels = [ids_to_labels_intent[id.item()] for id in eval_intent_labels]\n",
        "    v_intent_predictions = [ids_to_labels_intent[id.item()] for id in eval_intent_preds]\n",
        "\n",
        "    v_ner_loss = eval_ner_loss / nb_eval_steps\n",
        "    v_intent_loss = eval_intent_loss / nb_eval_steps\n",
        "    eval_ner_accuracy = eval_ner_accuracy / nb_eval_steps\n",
        "    eval_intent_accuracy = eval_intent_accuracy / nb_eval_steps\n",
        "    print(f\"Validation NER loss: {v_ner_loss}\")\n",
        "    print(f\"Validation INTENT loss: {v_intent_loss}\")\n",
        "    print(f\"Validation NER accuracy: {eval_ner_accuracy}\")\n",
        "    print(f\"Validation INTENT accuracy: {eval_intent_accuracy}\")\n",
        "\n",
        "    return v_ner_labels, v_ner_predictions, v_intent_labels, v_intent_predictions"
      ],
      "metadata": {
        "id": "sSumzyRcf4bc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v_ner_labels, v_ner_predictions, v_intent_labels, v_intent_predictions = eval(ner_model, intent_model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD4nCQVzpPep",
        "outputId": "eef95b81-6410-4bbf-fe9c-a4beb8e9c447"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation NER loss per 0 training steps: 0.4739464521408081\n",
            "Validation INTENT loss per 0 training steps: 1.371886134147644\n",
            "Validation NER loss per 5 training steps: 0.7722306847572327\n",
            "Validation INTENT loss per 5 training steps: 0.9412998557090759\n",
            "Validation NER loss per 10 training steps: 0.7568662762641907\n",
            "Validation INTENT loss per 10 training steps: 0.9561601281166077\n",
            "Validation NER loss per 15 training steps: 0.7189643383026123\n",
            "Validation INTENT loss per 15 training steps: 0.810491681098938\n",
            "Validation NER loss per 20 training steps: 0.6933809518814087\n",
            "Validation INTENT loss per 20 training steps: 0.760198712348938\n",
            "Validation NER loss: 0.7181467413902283\n",
            "Validation INTENT loss: 0.7319214940071106\n",
            "Validation NER accuracy: 0.8080488955488957\n",
            "Validation INTENT accuracy: 0.9166666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "print(classification_report(v_ner_labels, v_ner_predictions))"
      ],
      "metadata": {
        "id": "Fv6qk2pYppKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Mr_kyDspvN0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
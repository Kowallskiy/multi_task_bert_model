{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "lJDVuWO3B1pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CptExsPx7v1M"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForTokenClassification, BertForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import TextDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "id": "0xAaLdBZE0G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# Read the JSON file\n",
        "path = \"/content/drive/MyDrive/model/structured_data.json\"\n",
        "with open(path, 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Assuming the dataset is a list of dictionaries\n",
        "\n",
        "# Define a custom Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Create an instance of CustomDataset\n",
        "custom_dataset = CustomDataset(dataset)\n",
        "\n",
        "# Create a DataLoader\n",
        "batch_size = 1\n",
        "train_dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "bXc4PNVFRP4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = []\n",
        "for sample in train_dataloader:\n",
        "    key.extend(sample['entities'].keys())"
      ],
      "metadata": {
        "id": "sMcD55taOmXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = set(key)\n",
        "print(key)\n",
        "print(len(key))\n",
        "num_ner_labels = len(key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJB6otmMQSDG",
        "outputId": "84ab7548-1968-4751-b4a4-c6b07d760423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'task', 'specific_time', 'date', 'duration', 'days', 'time'}\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = []\n",
        "for sample in train_dataloader:\n",
        "    l.extend(sample['intent'])"
      ],
      "metadata": {
        "id": "UY9epjj3RVri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = set(l)\n",
        "print(l)\n",
        "print(len(l))\n",
        "num_intent_labels=len(l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAA0ljZPH0oL",
        "outputId": "fa385be0-6baf-4bca-c4ce-1253c748d856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"'Set Reminder'\", \"'Set Timer'\", \"'Set Alarm'\", \"'Schedule Appointment'\", \"'Schedule Meeting'\"}\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT tokenizer and models for token classification and sequence classification\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "ner_model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_ner_labels)\n",
        "intent_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_intent_labels)"
      ],
      "metadata": {
        "id": "SZaMCTlI71p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing the dataset"
      ],
      "metadata": {
        "id": "-odMqRP2VICq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# Read the JSON file\n",
        "path = \"/content/drive/MyDrive/model/structured_data.json\"\n",
        "with open(path, 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Assuming the dataset is a list of dictionaries\n",
        "\n",
        "# Define a custom Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Create an instance of CustomDataset\n",
        "custom_dataset = CustomDataset(dataset)\n",
        "\n",
        "# Create a DataLoader\n",
        "batch_size = 1\n",
        "train_dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "YrRLhKacVDml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs = tokenizer(train_dataloader['text'], padding=True, truncation=True, return_tensors='pt')\n",
        "# task -> B-TASK\n",
        "# date -> B-DATE\n",
        "# duration -> B-DUR\n",
        "# days -> B_DAY\n",
        "# time\n",
        "# specific_time\n",
        "label_ner_map = {'O': 0,}\n",
        "encoded_ner_labels = [[label_ner_map[label] for label in sent_label] for sent_label in train_dataloader['entities']]"
      ],
      "metadata": {
        "id": "DGFDqLQTVHcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "intent_labels = list(l)\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "encoded_intent_labels = label_encoder.fit_transform(intent_labels)"
      ],
      "metadata": {
        "id": "_fSSwvVtI1MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenizedDataset(Dataset):\n",
        "    def __init__(self, tokenized_inputs, encoded_ner_labels, encoded_intent_labels):\n",
        "        self.tokenized_inputs = tokenized_inputs\n",
        "        self.encoded_ner_labels = encoded_ner_labels\n",
        "        self.encoded_intent_labels = encoded_intent_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inputs = self.tokenized_inputs[idx]\n",
        "        ner_labels = self.encoded_ner_labels[idx]\n",
        "        intent_labels = self.encoded_intent_labels[idx]\n",
        "\n",
        "        return {\n",
        "            'inputs': inputs,\n",
        "            'ner_labels': ner_labels,\n",
        "            'intent_labels': intent_labels\n",
        "        }"
      ],
      "metadata": {
        "id": "LqdVj_pZo6P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cust_dataset = CustomDataset(tokenized_inputs, encoded_ner_labels, encoded_intent_labels)\n",
        "data_loader = DataLoader(cust_dataset, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "FSIViFHI1b48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TafoSlh3S8Bn",
        "outputId": "99626944-db75-4499-c23b-708e8806fbfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intent_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6niOzBtQS_Am",
        "outputId": "aace6bb3-584e-4a82-b0b1-43d9d6125bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in train_dataloader:\n",
        "    print(sample['entities'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO7vbAMi0R7H",
        "outputId": "48e83237-18db-40d6-bc1b-9e0c36ffe9ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'task': ['cooking'], 'duration': ['1 hour']}\n",
            "{'time': ['3 PM'], 'date': ['tomorrow']}\n",
            "{'duration': ['10 minutes']}\n",
            "{'task': ['5-minute meditation session'], 'duration': ['5 minutes']}\n",
            "{'task': ['webinar'], 'time': ['2 PM'], 'duration': ['2 days']}\n",
            "{'date': ['10th of next month'], 'time': ['2 PM']}\n",
            "{'task': ['20-minute workout session'], 'duration': ['20 minutes']}\n",
            "{'task': ['buy groceries'], 'date': ['Saturday'], 'time': ['afternoon']}\n",
            "{'task': ['study session'], 'duration': ['45 minutes']}\n",
            "{'time': ['7:30 AM'], 'date': ['tomorrow']}\n",
            "{'task': ['water the plants'], 'time': ['9 AM'], 'days': [('Tuesday',), ('Thursday',)]}\n",
            "{'task': ['pay bills'], 'date': ['last day of the month']}\n",
            "{'duration': ['30 minutes'], 'task': ['call John']}\n",
            "{'task': ['buy groceries'], 'date': ['tomorrow'], 'time': ['morning']}\n",
            "{'date': ['end of the month'], 'time': ['4:30 PM']}\n",
            "{'date': ['next Monday'], 'time': ['morning'], 'specific_time': ['10:30']}\n",
            "{'date': ['March 20th'], 'time': ['10:30 AM']}\n",
            "{'time': ['2:30 PM'], 'date': ['15th of this month']}\n",
            "{'task': ['15-minute break'], 'duration': ['15 minutes']}\n",
            "{'task': ['send the report'], 'time': ['4:30 PM'], 'date': ['today']}\n",
            "{'task': ['call Sarah'], 'date': ['next Wednesday'], 'time': ['afternoon']}\n",
            "{'task': ['send the report'], 'time': ['9 AM'], 'date': ['tomorrow']}\n",
            "{'task': ['pick up the laundry'], 'time': ['afternoon'], 'days': [('Friday',)]}\n",
            "{'time': ['9 AM'], 'date': ['next Friday']}\n",
            "{'date': ['next Friday'], 'time': ['noon']}\n",
            "{'task': ['project deadline'], 'time': ['5 PM'], 'date': ['Friday']}\n",
            "{'date': ['April 5th'], 'time': ['morning'], 'specific_time': ['11:00']}\n",
            "{'date': ['next Wednesday'], 'time': ['afternoon']}\n",
            "{'task': ['10-minute break'], 'duration': ['10 minutes']}\n",
            "{'task': ['presentation'], 'time': ['4 PM'], 'duration': ['3 hours'], 'date': ['today']}\n",
            "{'time': ['7 AM']}\n",
            "{'date': ['Monday']}\n",
            "{'date': ['May 15th'], 'time': ['evening']}\n",
            "{'time': ['6:45 AM']}\n",
            "{'date': ['first Monday of next month'], 'time': ['3 PM']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam([\n",
        "    {'params': ner_model.parameters()},\n",
        "    {'params': intent_model.parameters()}\n",
        "], lr=1e-5)  # Define optimizer for both models\n",
        "\n",
        "num_epochs = 3  # Define number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:  # Iterate over your dataset batches\n",
        "        inputs = tokenizer(batch['text'], return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "        # Forward pass for intent classification\n",
        "        outputs_intent = intent_model(**inputs)\n",
        "        intent_labels = batch['intent']\n",
        "        intent_loss = torch.nn.CrossEntropyLoss()(outputs_intent.logits, intent_labels)\n",
        "\n",
        "        # Forward pass for NER\n",
        "        outputs_ner = ner_model(**inputs)\n",
        "        ner_labels = batch['entities']\n",
        "        ner_loss = torch.nn.CrossEntropyLoss()(outputs_ner.logits.view(-1, num_ner_labels), ner_labels.view(-1))\n",
        "\n",
        "        # Total loss (you might adjust weights for different tasks)\n",
        "        total_loss = ner_loss + intent_loss\n",
        "\n",
        "        # Backpropagation\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "h9JGFaBu8wGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam([\n",
        "    {'params': ner_model.parameters()},\n",
        "    {'params': intent_model.parameters()}\n",
        "], lr=1e-5)  # Define optimizer for both models\n",
        "\n",
        "num_epochs = 3  # Define number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:  # Iterate over your dataset batches\n",
        "        inputs = tokenizer(batch['text'], return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "        # Forward pass for intent classification\n",
        "        outputs_intent = intent_model(**inputs)\n",
        "        intent_labels = batch['intent']\n",
        "        intent_loss = torch.nn.CrossEntropyLoss()(outputs_intent.logits, intent_labels)\n",
        "\n",
        "        # Forward pass for NER\n",
        "        outputs_ner = ner_model(**inputs)\n",
        "        ner_labels = batch['entities']\n",
        "        ner_loss = torch.nn.CrossEntropyLoss()(outputs_ner.logits.view(-1, num_ner_labels), ner_labels.view(-1))\n",
        "\n",
        "        # Total loss (you might adjust weights for different tasks)\n",
        "        total_loss = ner_loss + intent_loss\n",
        "\n",
        "        # Backpropagation\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        # No idea yet"
      ],
      "metadata": {
        "id": "MZBdetDs9xfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the Multi-Task Architecture"
      ],
      "metadata": {
        "id": "OZ5Jw2arAaVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTaskBertWrapper(nn.Module):\n",
        "    def __init__(self, ner_model, intent_model):\n",
        "        super().__init__()\n",
        "        self.ner_model = ner_model\n",
        "        self.intent_model = intent_model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None, task_type=None):\n",
        "        if task_type = 'ner':\n",
        "            self.ner_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        elif task_type = 'intent':\n",
        "            self.intent_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        else:\n",
        "            raise ValueError('Invalid task_type. Use \"ner\" or \"intent\"')"
      ],
      "metadata": {
        "id": "afndjDPHAeHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiTaskBertWrapper(ner_model, intent_model)"
      ],
      "metadata": {
        "id": "tLfiD2uA50ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "5coZ3SS3_ryJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = list(model.ner_model.parameters()) + list(model.intent_model.parameters())\n",
        "optimizer = torch.optim.AdamW(params, lr=1e-5)\n",
        "\n",
        "ner_loss_function = torch.nn.CrossEntropyLoss()\n",
        "intent_loss_function = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "YSzomyoc_s7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        inputs = batch['text']\n",
        "        attention_masks = batch['attention_mask']\n",
        "        labels_ner = batch['entities']\n",
        "        labels_intent = batch['intent']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass for NER task\n",
        "        ner_outputs = model.ner_model(input_ids=inputs, attention_mask=attention_masks, labels=labels_ner, task_type='ner')\n",
        "        ner_loss = ner_loss_function(ner_outputs.logits.view(-1, num_ner_labels), labels_ner.view(-1))\n",
        "\n",
        "        ner_loss.backward()\n",
        "\n",
        "        # Forward pass for Intent Classification task\n",
        "        intent_outputs = model.intent_model(input_ids=inputs, attention_mask=attention_masks, labels=labels_intent, task_type='intent')\n",
        "        intent_loss = intent_loss_function(intent_outputs.logits.view(-1, num_intent_labels), labels_intent.view(-1))\n",
        "\n",
        "        intent_loss.backward()\n",
        "\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "-kwfmmiGAKtX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "lJDVuWO3B1pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CptExsPx7v1M"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForTokenClassification, BertForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import TextDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "id": "0xAaLdBZE0G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# Read the JSON file\n",
        "path = \"/content/drive/MyDrive/model/B_data.json\"\n",
        "with open(path, 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Assuming the dataset is a list of dictionaries\n",
        "\n",
        "# Define a custom Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Create an instance of CustomDataset\n",
        "custom_dataset = CustomDataset(dataset)\n",
        "\n",
        "# Create a DataLoader\n",
        "batch_size = 1\n",
        "train_dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "bXc4PNVFRP4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = []\n",
        "for sample in train_dataloader:\n",
        "    key.append(sample['entities'])"
      ],
      "metadata": {
        "id": "sMcD55taOmXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities = []\n",
        "for substr in key:\n",
        "    tokens = substr[0].split()\n",
        "    entities.extend(tokens)"
      ],
      "metadata": {
        "id": "RuS23pIA8o0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities"
      ],
      "metadata": {
        "id": "KRXA6PeKCy4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = set(entities)\n",
        "print(key)\n",
        "print(len(key))\n",
        "num_ner_labels = len(key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJB6otmMQSDG",
        "outputId": "78a9f166-9a2c-4166-9a77-11f448d11ab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'I-TASK', 'O', 'B-TIME', 'B-DATE', 'B-TASK', 'B-DUR', 'I-TIME', 'I-DUR', 'I-DATE'}\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = []\n",
        "for sample in train_dataloader:\n",
        "    l.extend(sample['intent'])"
      ],
      "metadata": {
        "id": "UY9epjj3RVri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = set(l)\n",
        "print(l)\n",
        "print(len(l))\n",
        "num_intent_labels=len(l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAA0ljZPH0oL",
        "outputId": "41860064-6fc4-47a9-8889-9de79a3699e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"'Set Timer'\", \"'Set Alarm'\", \"'Set Reminder'\", \"'Schedule Meeting'\", \"'Schedule Appointment'\"}\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT tokenizer and models for token classification and sequence classification\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "ner_model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_ner_labels)\n",
        "intent_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_intent_labels)"
      ],
      "metadata": {
        "id": "SZaMCTlI71p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing the dataset"
      ],
      "metadata": {
        "id": "-odMqRP2VICq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maybe I should try to set trincation and padding to False."
      ],
      "metadata": {
        "id": "n7tTgv64JFcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs = []\n",
        "\n",
        "max_sequence_length = 0  # Initialize maximum sequence length\n",
        "\n",
        "for batch in train_dataloader:\n",
        "    text = batch['text']\n",
        "    batch_tokenized = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
        "    input_ids = batch_tokenized['input_ids']\n",
        "    attention_mask = batch_tokenized['attention_mask']\n",
        "    tokenized_inputs.append((input_ids, attention_mask))\n",
        "\n",
        "    # Update maximum sequence length\n",
        "    max_sequence_length = max(max_sequence_length, input_ids.shape[1])\n",
        "\n",
        "# Pad sequences to the maximum sequence length\n",
        "for i in range(len(tokenized_inputs)):\n",
        "    input_ids, attention_mask = tokenized_inputs[i]\n",
        "    pad_length = max_sequence_length - input_ids.shape[1]\n",
        "    padded_input_ids = torch.nn.functional.pad(input_ids, (0, pad_length), value=tokenizer.pad_token_id)\n",
        "    padded_attention_mask = torch.nn.functional.pad(attention_mask, (0, pad_length), value=0)  # Assuming 0 for padding mask\n",
        "    tokenized_inputs[i] = (padded_input_ids, padded_attention_mask)\n",
        "\n",
        "# Concatenate input IDs and attention masks separately\n",
        "input_ids = torch.cat([tensor[0] for tensor in tokenized_inputs], dim=0)\n",
        "attention_mask = torch.cat([tensor[1] for tensor in tokenized_inputs], dim=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "DGFDqLQTVHcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids"
      ],
      "metadata": {
        "id": "VpCPdPOoJAqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# task -> B-TASK\n",
        "# date -> B-DATE\n",
        "# duration -> B-DUR\n",
        "# days -> B_DAY\n",
        "# time -> B-TIME\n",
        "# specific_time\n",
        "encoded_ner_labels = []\n",
        "label_ner_map = {'O': 0, 'B-DATE': 1, 'I-DATE': 2, 'B-TIME': 3, 'I-TIME': 4, 'B-TASK': 5, 'I-TASK': 6, 'B-DUR': 7, 'I-DUR': 8}\n",
        "for sample in train_dataloader:\n",
        "    encoded_ner_labels.extend([label_ner_map[label] for label in sent_label.split()] for sent_label in sample['entities'])\n",
        "encoded_ner_labels"
      ],
      "metadata": {
        "id": "Y6J-hyyTpSAD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c46f5e5b-899e-4cd7-e247-2e4c0aae7331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 0, 0, 7, 8],\n",
              " [0, 0, 0, 0, 0, 0, 3, 4, 1],\n",
              " [0, 0, 0, 0, 1, 2, 0, 3, 4],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 3, 4],\n",
              " [0, 0, 0, 0, 3, 4],\n",
              " [0, 0, 0, 5, 6, 0, 7, 8],\n",
              " [0, 0, 0, 0, 1, 2, 3],\n",
              " [0, 0, 0, 0, 0, 0, 5, 0, 7, 8],\n",
              " [0, 0, 0, 0, 5, 6, 0, 3, 4, 0, 1],\n",
              " [0, 0, 0, 0, 0, 1, 2, 0, 3, 4],\n",
              " [0, 0, 0, 0, 0, 3, 5],\n",
              " [0, 0, 0, 5, 6, 1, 3],\n",
              " [0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0, 3, 4]]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train_dataloader:\n",
        "    print(i['entities'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db3lumxzUty7",
        "outputId": "a792ca63-7bf0-4e61-9fd7-3852730956fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O O O O B-DUR I-DUR']\n",
            "['O O O O O O B-TIME I-TIME B-DATE']\n",
            "['O O O O B-DATE I-DATE O B-TIME I-TIME']\n",
            "['O O O O O O O O O O B-DATE']\n",
            "['O O O O O O O O B-DATE I-DATE I-DATE I-DATE O B-TIME I-TIME']\n",
            "['O O O O B-TIME I-TIME']\n",
            "['O O O B-TASK I-TASK O B-DUR I-DUR']\n",
            "['O O O O B-DATE I-DATE B-TIME']\n",
            "['O O O O O O B-TASK O B-DUR I-DUR']\n",
            "['O O O O B-TASK I-TASK O B-TIME I-TIME O B-DATE']\n",
            "['O O O O O B-DATE I-DATE O B-TIME I-TIME']\n",
            "['O O O O O B-TIME B-TASK']\n",
            "['O O O B-TASK I-TASK B-DATE B-TIME']\n",
            "['O O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O B-TIME I-TIME']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "sample_entities = []\n",
        "for sample in train_dataloader:\n",
        "    sample_entities.extend(sample['entities'])\n",
        "\n",
        "# Flatten the list of strings into a single list of tokens\n",
        "all_entities = [entity for sample in sample_entities for entity in sample.split()]\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit LabelEncoder to all unique entity labels\n",
        "label_encoder.fit(all_entities)\n",
        "\n",
        "# Encode entities for each sample in sample_entities\n",
        "encoded_entities = [\n",
        "    label_encoder.transform(entity.split()).tolist() for entity in sample_entities\n",
        "]\n",
        "\n",
        "print(encoded_entities)\n"
      ],
      "metadata": {
        "id": "TdU2s6l0pUHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6ba95c0-4067-4a80-beb8-f273fdd8a657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[8, 8, 8, 8, 1, 5], [8, 8, 8, 8, 8, 8, 3, 7, 0], [8, 8, 8, 8, 0, 4, 8, 3, 7], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0], [8, 8, 8, 8, 8, 8, 8, 8, 0, 4, 4, 4, 8, 3, 7], [8, 8, 8, 8, 3, 7], [8, 8, 8, 2, 6, 8, 1, 5], [8, 8, 8, 8, 0, 4, 3], [8, 8, 8, 8, 8, 8, 2, 8, 1, 5], [8, 8, 8, 8, 2, 6, 8, 3, 7, 8, 0], [8, 8, 8, 8, 8, 0, 4, 8, 3, 7], [8, 8, 8, 8, 8, 3, 2], [8, 8, 8, 2, 6, 0, 3], [8, 8, 8, 8, 8, 8, 0, 4, 4, 4, 4, 8, 3, 7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intent_labels = list(l)\n",
        "\n",
        "encoded_intent_labels = label_encoder.fit_transform(intent_labels)"
      ],
      "metadata": {
        "id": "_fSSwvVtI1MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_intent_labels"
      ],
      "metadata": {
        "id": "N-1aazbaCMFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenizedDataset(Dataset):\n",
        "    def __init__(self, tokenized_inputs, encoded_ner_labels, encoded_intent_labels):\n",
        "        self.tokenized_inputs = tokenized_inputs\n",
        "        self.encoded_ner_labels = encoded_ner_labels\n",
        "        self.encoded_intent_labels = encoded_intent_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inputs = self.tokenized_inputs[idx]\n",
        "        ner_labels = self.encoded_ner_labels[idx]\n",
        "        intent_labels = self.encoded_intent_labels[idx]\n",
        "\n",
        "        return {\n",
        "            'inputs': inputs,\n",
        "            'ner_labels': ner_labels,\n",
        "            'intent_labels': intent_labels\n",
        "        }"
      ],
      "metadata": {
        "id": "LqdVj_pZo6P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cust_dataset = CustomDataset(tokenized_inputs, encoded_ner_labels, encoded_intent_labels)\n",
        "data_loader = DataLoader(cust_dataset, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "FSIViFHI1b48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "52671765-7348-4f7e-aff3-c14526261e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-d2115cbc3401>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcust_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_ner_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_intent_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcust_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: CustomDataset.__init__() takes 2 positional arguments but 4 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ner_model architecture\n",
        "ner_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TafoSlh3S8Bn",
        "outputId": "99626944-db75-4499-c23b-708e8806fbfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intent_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6niOzBtQS_Am",
        "outputId": "aace6bb3-584e-4a82-b0b1-43d9d6125bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in train_dataloader:\n",
        "    print(sample['entities'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO7vbAMi0R7H",
        "outputId": "4df311a5-acce-4ad1-f5f4-2dc90c12fb31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O O O O B-DUR I-DUR']\n",
            "['O O O O O O B-TIME I-TIME B-DATE']\n",
            "['O O O O B-DATE I-DATE O B-TIME I-TIME']\n",
            "['O O O O O O O O O O B-DATE']\n",
            "['O O O O O O O O B-DATE I-DATE I-DATE I-DATE O B-TIME I-TIME']\n",
            "['O O O O B-TIME I-TIME']\n",
            "['O O O B-TASK I-TASK O B-DUR I-DUR']\n",
            "['O O O O B-DATE I-DATE B-TIME']\n",
            "['O O O O O O B-TASK O B-DUR I-DUR']\n",
            "['O O O O B-TASK I-TASK O B-TIME I-TIME O B-DATE']\n",
            "['O O O O O B-DATE I-DATE O B-TIME I-TIME']\n",
            "['O O O O O B-TIME B-TASK']\n",
            "['O O O B-TASK I-TASK B-DATE B-TIME']\n",
            "['O O O O O O B-DATE I-DATE I-DATE I-DATE I-DATE O B-TIME I-TIME']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam([\n",
        "    {'params': ner_model.parameters()},\n",
        "    {'params': intent_model.parameters()}\n",
        "], lr=1e-5)  # Define optimizer for both models\n",
        "\n",
        "num_epochs = 3  # Define number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:  # Iterate over your dataset batches\n",
        "        inputs = tokenizer(batch['text'], return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "        # Forward pass for intent classification\n",
        "        outputs_intent = intent_model(**inputs)\n",
        "        intent_labels = batch['intent']\n",
        "        intent_loss = torch.nn.CrossEntropyLoss()(outputs_intent.logits, intent_labels)\n",
        "\n",
        "        # Forward pass for NER\n",
        "        outputs_ner = ner_model(**inputs)\n",
        "        ner_labels = batch['entities']\n",
        "        ner_loss = torch.nn.CrossEntropyLoss()(outputs_ner.logits.view(-1, num_ner_labels), ner_labels.view(-1))\n",
        "\n",
        "        # Total loss (you might adjust weights for different tasks)\n",
        "        total_loss = ner_loss + intent_loss\n",
        "\n",
        "        # Backpropagation\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "h9JGFaBu8wGT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "2f494897-acf9-4ae6-d440-8b65908dd289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-34c0298719de>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moutputs_intent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintent_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mintent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intent'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mintent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_intent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintent_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Forward pass for NER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3053\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'target' (position 2) must be Tensor, not list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam([\n",
        "    {'params': ner_model.parameters()},\n",
        "    {'params': intent_model.parameters()}\n",
        "], lr=1e-5)  # Define optimizer for both models\n",
        "\n",
        "num_epochs = 3  # Define number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:  # Iterate over your dataset batches\n",
        "        inputs = tokenizer(batch['text'], return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "        # Forward pass for intent classification\n",
        "        outputs_intent = intent_model(**inputs)\n",
        "        intent_labels = batch['intent']\n",
        "        intent_loss = torch.nn.CrossEntropyLoss()(outputs_intent.logits, intent_labels)\n",
        "\n",
        "        # Forward pass for NER\n",
        "        outputs_ner = ner_model(**inputs)\n",
        "        ner_labels = batch['entities']\n",
        "        ner_loss = torch.nn.CrossEntropyLoss()(outputs_ner.logits.view(-1, num_ner_labels), ner_labels.view(-1))\n",
        "\n",
        "        # Total loss (you might adjust weights for different tasks)\n",
        "        total_loss = ner_loss + intent_loss\n",
        "\n",
        "        # Backpropagation\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        # No idea yet"
      ],
      "metadata": {
        "id": "MZBdetDs9xfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the Multi-Task Architecture"
      ],
      "metadata": {
        "id": "OZ5Jw2arAaVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTaskBertWrapper(nn.Module):\n",
        "    def __init__(self, ner_model, intent_model):\n",
        "        super().__init__()\n",
        "        self.ner_model = ner_model\n",
        "        self.intent_model = intent_model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None, task_type=None):\n",
        "        if task_type = 'ner':\n",
        "            self.ner_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        elif task_type = 'intent':\n",
        "            self.intent_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        else:\n",
        "            raise ValueError('Invalid task_type. Use \"ner\" or \"intent\"')"
      ],
      "metadata": {
        "id": "afndjDPHAeHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiTaskBertWrapper(ner_model, intent_model)"
      ],
      "metadata": {
        "id": "tLfiD2uA50ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "5coZ3SS3_ryJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = list(model.ner_model.parameters()) + list(model.intent_model.parameters())\n",
        "optimizer = torch.optim.AdamW(params, lr=1e-5)\n",
        "\n",
        "ner_loss_function = torch.nn.CrossEntropyLoss()\n",
        "intent_loss_function = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "YSzomyoc_s7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        inputs = batch['text']\n",
        "        attention_masks = batch['attention_mask']\n",
        "        labels_ner = batch['entities']\n",
        "        labels_intent = batch['intent']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass for NER task\n",
        "        ner_outputs = model.ner_model(input_ids=inputs, attention_mask=attention_masks, labels=labels_ner, task_type='ner')\n",
        "        ner_loss = ner_loss_function(ner_outputs.logits.view(-1, num_ner_labels), labels_ner.view(-1))\n",
        "\n",
        "        ner_loss.backward()\n",
        "\n",
        "        # Forward pass for Intent Classification task\n",
        "        intent_outputs = model.intent_model(input_ids=inputs, attention_mask=attention_masks, labels=labels_intent, task_type='intent')\n",
        "        intent_loss = intent_loss_function(intent_outputs.logits.view(-1, num_intent_labels), labels_intent.view(-1))\n",
        "\n",
        "        intent_loss.backward()\n",
        "\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "-kwfmmiGAKtX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}